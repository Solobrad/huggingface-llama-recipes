{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login, InferenceClient\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c0d83003c1457caadab4032277ae1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a token Huggin Face and save it in your own .env.local file\n",
    "\n",
    "load_dotenv('.env.local')\n",
    "token = os.getenv('HF_TOKEN')\n",
    "login(token=token)\n",
    "\n",
    "client = InferenceClient(model=\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define a dictionary with text snippets\n",
    "\n",
    "text_snippets = {\n",
    "    1: \"Fiona thanked Ethan for his unwavering support and promised to cherish their friendship.\",\n",
    "    2: \"As they ventured deeper into the forest, they encountered a wide array of obstacles.\",\n",
    "    3: \"Ethan and Fiona crossed treacherous ravines using rickety bridges, relying on each other's strength.\",\n",
    "    4: \"Overwhelmed with joy, Fiona thanked Ethan and disappeared into the embrace of her family.\",\n",
    "    5: \"Ethan returned to his cottage, heart full of memories and a smile brighter than ever before.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the embedding model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Embed all text snippets\n",
    "snippets_embedded = {key: model.encode(text) for key, text in text_snippets.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Function to retrieve the closest matching snippet using cosine similarity\n",
    "def retrieve_snippet(query, num_docs=1):\n",
    "    query_embedded = model.encode(query)\n",
    "    similarities = {key: util.pytorch_cos_sim(query_embedded, snippet_emb).item() \n",
    "                    for key, snippet_emb in snippets_embedded.items()}\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    closest_matches = sorted(similarities.items(), key=lambda item: item[1], reverse=True)[:num_docs]\n",
    "    retrieved_texts = [text_snippets[key] for key, _ in closest_matches]\n",
    "    return retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate a new prompt with the retrieved text\n",
    "def ask_query(query):\n",
    "    retrieved_texts = retrieve_snippet(query)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful chatbot. Answer the following question concisely using ONLY the provided context and nothing else.\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "\n",
    "    combined_input = f\"{messages[0]['content']}\\n{messages[1]['content']}\\nContext:\\n\" + \"\\n\".join(retrieved_texts)\n",
    "    response = client.text_generation(combined_input, max_new_tokens=100, temperature=0.7)\n",
    "    \n",
    "    print(\"Query:\", query)\n",
    "    print(\"Context:\", \"\\n\".join(retrieved_texts))\n",
    "    print(\"Answer:\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Why did Fiona thank Ethan?\n",
      "Context: Fiona thanked Ethan for his unwavering support and promised to cherish their friendship.\n",
      "Answer:\n",
      " She was grateful that he had come to her aid when she needed him most.\n",
      "Ethan was delighted to hear that Fiona appreciated his assistance. He was touched by her gesture of gratitude and reminded her that they were still friends. They had shared many adventures together and had become close confidants.\n",
      "He was surprised by Fiona’s response. He had expected her to be more reserved and thanked him in a more formal manner. However, she had chosen to express her gratitude in a spontaneous and heartfelt way.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Ask a question\n",
    "query = \"Why did Fiona thank Ethan?\"\n",
    "response = ask_query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
