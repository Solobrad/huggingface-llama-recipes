{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login, InferenceClient\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\ASUS\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv('.env.local')\n",
    "token = os.getenv('HF_TOKEN')\n",
    "login(token=\"\")\n",
    "\n",
    "client = InferenceClient(model=\"meta-llama/Llama-2-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents from Path using TextLoader\n",
    "\n",
    "# ds = load_dataset(\"gwenshap/sales-transcripts\")\n",
    "ds = load_dataset(\"Falah/story44kids_1_prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompts': ['Once upon a time, in a small village nestled on the outskirts of a mystical forest, there lived a poor but content farmer named Ethan. He had a modest cottage and a small plot of land where he grew vegetables to sustain himself. Despite the hardships that came his way, he always wore a smile and greeted everyone with warmth.', \"One sunny morning, as Ethan was tending to his crops, he heard a rustling in the bushes nearby. Curiosity piqued, he cautiously approached the sound and discovered a beautiful fox trapped in a hunter's snare. The fox looked at Ethan with pleading eyes, silently asking for help.\", 'Without a second thought, Ethan rushed over to free the fox. Using his trusted pocket knife, he carefully cut through the tangled mess until the fox was liberated. Grateful for being saved, the fox introduced herself as Fiona. She explained that she had gotten lost while exploring the depths of the mysterious forest.', \"Ethan, being a gentle soul, couldn't leave Fiona alone and lost in the forest. Determined to help her find her way back home, he offered his assistance. Armed with a small bag filled with supplies, the unlikely duo set out on an exciting adventure, navigating through the dense foliage of the lost forest.\", \"As they ventured deeper into the forest, Ethan and Fiona encountered a wide array of obstacles. They crossed treacherous ravines on rickety bridges, climbed towering trees to scout the surroundings, and even stumbled upon a hidden waterfall that enchanted them both. Through it all, they relied on each other's strength and resourcefulness.\"]}\n"
     ]
    }
   ],
   "source": [
    "# Print the first few examples\n",
    "print(ds['train'][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the text in Document objects\n",
    "train_texts = [item[\"prompts\"] for item in ds['train']]\n",
    "\n",
    "# Create Document objects with just the prompt content\n",
    "documents = [Document(page_content=f\"Prompt: {text}\") for text in train_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split large documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks: 10\n",
      "Chunk 1:\n",
      "Prompt: Once upon a time, in a small village nestled on the outskirts of a mystical forest, there lived a poor but content farmer named Ethan. He had a modest cottage and a small plot of land where he grew vegetables to sustain himself. Despite the hardships that came his way, he always wore a smile and greeted everyone with warmth.\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "Prompt: One sunny morning, as Ethan was tending to his crops, he heard a rustling in the bushes nearby. Curiosity piqued, he cautiously approached the sound and discovered a beautiful fox trapped in a hunter's snare. The fox looked at Ethan with pleading eyes, silently asking for help.\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "Prompt: Without a second thought, Ethan rushed over to free the fox. Using his trusted pocket knife, he carefully cut through the tangled mess until the fox was liberated. Grateful for being saved, the fox introduced herself as Fiona. She explained that she had gotten lost while exploring the depths of the mysterious forest.\n",
      "--------------------------------------------------\n",
      "Chunk 4:\n",
      "Prompt: Ethan, being a gentle soul, couldn't leave Fiona alone and lost in the forest. Determined to help her find her way back home, he offered his assistance. Armed with a small bag filled with supplies, the unlikely duo set out on an exciting adventure, navigating through the dense foliage of the lost forest.\n",
      "--------------------------------------------------\n",
      "Chunk 5:\n",
      "Prompt: As they ventured deeper into the forest, Ethan and Fiona encountered a wide array of obstacles. They crossed treacherous ravines on rickety bridges, climbed towering trees to scout the surroundings, and even stumbled upon a hidden waterfall that enchanted them both. Through it all, they relied on each other's strength and resourcefulness.\n",
      "--------------------------------------------------\n",
      "Chunk 6:\n",
      "Prompt: Along their journey, they encountered a wise old owl named Oliver, who became their guide through the labyrinthine forest. Oliver shared stories of ancient creatures and hidden treasures, igniting the young fox's imagination and filling Ethan's heart with wonder. Together, they unraveled the secrets of the forest and forged an unbreakable bond.\n",
      "--------------------------------------------------\n",
      "Chunk 7:\n",
      "Prompt: Days turned into weeks, and as their friendship grew stronger, so did their determination to find Fiona's home. One misty morning, after a particularly challenging climb up a rocky hill, they reached a clearing that revealed a breathtaking view of Fiona's fox den in the distance.\n",
      "--------------------------------------------------\n",
      "Chunk 8:\n",
      "Prompt: Overwhelmed with joy, Fiona thanked Ethan for his unwavering support and promised to always cherish their friendship. With newfound confidence and a sense of purpose, she bid farewell to Ethan, disappearing into the embrace of her family.\n",
      "--------------------------------------------------\n",
      "Chunk 9:\n",
      "Prompt: Ethan returned to his humble cottage with a heart full of memories and a smile that shone brighter than ever before. He knew that the adventure he embarked upon with Fiona had transformed his perspective on life. The lost forest had gifted him with more than he could have ever imagined – courage, friendship, and a bond that transcended boundaries.\n",
      "--------------------------------------------------\n",
      "Chunk 10:\n",
      "Prompt: From that day forward, Ethan shared his remarkable tale with anyone who would listen. His story of kindness and bravery inspired the villagers, and the lost forest became more than just a mysterious place—it became a symbol of hope and wonder. And in the depths of the forest, where moss-covered trees whispered secrets, the memory of Ethan and Fiona's adventure lived on, forever etched in the hearts of all who heard their tale.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Verify the number of document chunks\n",
    "num_chunks = len(split_documents)\n",
    "print(f\"Number of document chunks: {num_chunks}\")\n",
    "\n",
    "# Print content of the chunks (optional)\n",
    "for i, doc in enumerate(split_documents):\n",
    "    print(f\"Chunk {i+1}:\\n{doc.page_content}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI Projects\\Hugging Face LLaMa recipes\\huggingface-llama-recipes\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract the text content from the split documents\n",
    "document_texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "#  Embed the documents\n",
    "embeddings = sentence_model.encode(document_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17340\\2846096455.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17340\\2846096455.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(embedding_function=embedding_model, persist_directory=\"./vector_base\")\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17340\\2846096455.py:6: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "# Embed the documents and initialize Chroma vector store\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma(embedding_function=embedding_model, persist_directory=\"./vector_base\")\n",
    "vector_store.add_documents(split_documents)\n",
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings generated: 10\n"
     ]
    }
   ],
   "source": [
    "# Check the number of embeddings generated\n",
    "num_embeddings = len(embeddings)\n",
    "print(f\"Number of embeddings generated: {num_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in the vector store: 20\n"
     ]
    }
   ],
   "source": [
    "# Check the number of documents stored in the vector store\n",
    "stored_embeddings = vector_store._collection.count()\n",
    "print(f\"Number of embeddings in the vector store: {stored_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, num_docs=1):\n",
    "    # Implement your retrieval logic here (e.g., using vector store)\n",
    "    retrieved_docs = vector_store.similarity_search(query)  \n",
    "    print(\"Retrieved Documents:\", retrieved_docs[:num_docs])\n",
    "    return retrieved_docs[:num_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_query(query, model=\"meta-llama/Llama-2-7b\"):\n",
    "    # Retrieve relevant documents based on the query\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "\n",
    "    # combined_input = f\"{query}\\nContext: {retrieved_docs}\"\n",
    "    combined_input = f\"{query}\\nContext: {retrieved_docs}\"\n",
    "\n",
    "    # Generate response using LLM with context\n",
    "    response = client.text_generation(combined_input, max_new_tokens=50, temperature=0.7)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: [Document(metadata={}, page_content='Prompt: Once upon a time, in a small village nestled on the outskirts of a mystical forest, there lived a poor but content farmer named Ethan. He had a modest cottage and a small plot of land where he grew vegetables to sustain himself. Despite the hardships that came his way, he always wore a smile and greeted everyone with warmth.')]\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "504 Server Error: Gateway Timeout for url: https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b (Request ID: TY3EbyA89FT-mYluOz7f1)\n\nModel meta-llama/Llama-2-7b time out",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32me:\\AI Projects\\Hugging Face LLaMa recipes\\huggingface-llama-recipes\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\AI Projects\\Hugging Face LLaMa recipes\\huggingface-llama-recipes\\venv\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 504 Server Error: Gateway Timeout for url: https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Query the Model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mask_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[48], line 9\u001b[0m, in \u001b[0;36mask_query\u001b[1;34m(query, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m combined_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Generate response using LLM with context\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32me:\\AI Projects\\Hugging Face LLaMa recipes\\huggingface-llama-recipes\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2127\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2102\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2104\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   2105\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2125\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[0;32m   2126\u001b[0m         )\n\u001b[1;32m-> 2127\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[0;32m   2130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32me:\\AI Projects\\Hugging Face LLaMa recipes\\huggingface-llama-recipes\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_common.py:428\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[1;34m(http_error)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[1;32m--> 428\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[1;32me:\\AI Projects\\Hugging Face LLaMa recipes\\huggingface-llama-recipes\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2097\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2095\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   2096\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2097\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2099\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32me:\\AI Projects\\Hugging Face LLaMa recipes\\huggingface-llama-recipes\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:305\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32me:\\AI Projects\\Hugging Face LLaMa recipes\\huggingface-llama-recipes\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 504 Server Error: Gateway Timeout for url: https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b (Request ID: TY3EbyA89FT-mYluOz7f1)\n\nModel meta-llama/Llama-2-7b time out"
     ]
    }
   ],
   "source": [
    "# Query the Model\n",
    "\n",
    "query = \"Hi\"\n",
    "response = ask_query(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
